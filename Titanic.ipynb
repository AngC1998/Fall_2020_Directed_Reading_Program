{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=300, figsize=(9,3))\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Titanic Survival Estimation via Naïve Bayes</h1>\n",
    "<h2 align=\"center\">Angela Cao</h2>\n",
    "<h5 align=\"center\">Fall 2020 Directed Readings Program, University of Texas at Austin Department of Mathematics</h4>\n",
    "<h5 align=\"center\">Advised by Shane McQuarrie</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [**Introduction: The Titanic Problem**](#Introduction:-The-Titanic-Problem)\n",
    "2. [**Brief Data Summary**](#Brief-Data-Summary)\n",
    "3. [**The Naïve Bayes Algorithm**](#The-Naïve-Bayes-Algorithm)\n",
    "4. [**Applying the Algorithm**](#Applying-the-Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction: The Titanic Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On April 15, 1912, the [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) sank, leaving about 1,500 people dead. The goal of this project is to construct a machine learning algorithm to predict whether or not a person with given characteristics would have been likely to survive the accident. This is a [popular problem](https://www.kaggle.com/c/titanic) for a first-time exposure to machine learning.\n",
    "\n",
    "We have a [data set](https://www.kaggle.com/c/titanic/data) with information about 1,308 of the Titanic passengers, with the following features for each passenger. We begin by loading the data and displaying a few random entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from file.\n",
    "titanic_original = pd.read_csv(\"titanic.csv\")\n",
    "titanic_original.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which of these features differentiate a survivor from a nonsurvivor? We restrict our attention to the following features:\n",
    "- `Sex`: the sex of the passenger, either `male` or `female`.\n",
    "- `Age`: the age of the passenger, usually recorded as a positive integer.\n",
    "- `Pclass`: the class of the passenger's ticket, 1 for first class (the best), 2 for second class, and 3 for third class (the worst).\n",
    "- `Fare`: the cost of the passenger's ticket.\n",
    "\n",
    "Some of the other columns in the data may be useful, but certainly these features are more important survival indicators than, for example, the passenger `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the relevant columns.\n",
    "titanic = titanic_original[[\"Survived\", \"Sex\", \"Age\", \"Pclass\", \"Fare\"]]\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before any analysis, we split the data into _training_ and _testing_ sets, reserving 15% of the entries for testing. We are only allowed to use the testing set to evaluate the performance of our algorithm later on. Since some of the entries of the data may be missing (for example, some passengers don't have a recorded age), we replace missing values of the test data with the column averages of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "train_data, test_data = train_test_split(titanic, test_size=.15)\n",
    "print(f\"{len(train_data)} train points, {len(test_data)} test points ({len(titanic)} total)\")\n",
    "\n",
    "# Separate the Survival labels from the test data and replace\n",
    "# missing values with the averages from the training data.\n",
    "test_labels = test_data[\"Survived\"]\n",
    "test_data = test_data.drop(\"Survived\", axis=1).fillna(train_data.mean())\n",
    "\n",
    "test_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now give a brief statistical overview of the training data (remember, we aren't allowed to even look at the testing data). First, what was the rate of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 2))\n",
    "\n",
    "survival_count = train_data.groupby(\"Survived\").count().max(axis=1)\n",
    "survival_count.plot(kind=\"barh\", ax=ax)\n",
    "\n",
    "ax.set_ylabel(\"\"); ax.set_yticklabels([\"perished\", \"survived\"])\n",
    "ax.set_xlabel(\"number of passengers\")\n",
    "ax.set_title(f\"Passenger Survival Rate: {survival_count[1] / sum(survival_count):.2%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The survival rate (according to the training data) is around 40%. Without any further information about the passengers, we could assign every passenger in the test set a 40% survival probability. But we want survival probabilities that are tailored to each individual based on their individual features; that's the goal of our algorithm.\n",
    "\n",
    "Note that both groups, survivors and non-survivors, have several hundred members. These are large enough populations for us to legitimately consider an algorithm that relies on constructing approximate statistical distributions for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(9,2))\n",
    "\n",
    "train_data[\"Age\"].plot(kind=\"hist\", bins=80, ax=ax1)\n",
    "ax1.set_xlabel(\"passenger age\")\n",
    "\n",
    "train_data[\"Fare\"].plot(kind=\"hist\", bins=40, ax=ax2)\n",
    "ax2.set_ylabel(\"\"); ax2.set_xlabel(\"ticket fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The histogram on the left shows the distribution of the ages of the passengers in the training data. There are many passengers in their 20's and 30's, few teenagers, and a sizeable number of children. The distribution is slightly right-skewed, but it could be approximated fairly well with a normal (Gaussian) distribution.\n",
    "\n",
    "The histogram on the right shows the distribution of the ticket prices of the passengers in the training data. This distribution is extremely right-skewed: the overwhelming majority of passengers having tickets that cost \\$100  or less, but a few outliers have far more expensive tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we briefly look for statistical differences between survivors and nonsurvivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data.groupby([\"Survived\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This table sorts the data by the survival label (0 for non-survivors and 1 for survivors), then computes mean of the features for each group. A few things are immediately apparent:\n",
    "- Survivors had, on average, significantly more expensive tickets than non-survivors. This is due at least in part to the presence of the outliers observed above in the ticket price distribution.\n",
    "- Survivors, on average, were in a (slightly) higher class than non-survivors (recall Pclass = 1 means a first-class ticket). In other words, holding all other features equal, first-class passengers are more likely to survive than second-class passengers.\n",
    "- The average ages for survivors and non-survivors are relatively similar, with survivors being just two years younger than non-survirors on average.\n",
    "\n",
    "In short, the survivors and non-survivors exhibit different statistics; we aim to use that information to construct a \"survival classifier.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naïve Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our goal is to construct a function that maps a given tuple of `Sex`, `Age`, `Pclass`, and `Fare` values to a `Survival` value (0 for perished or 1 for survived).\n",
    "There are many ways to construct such a function, but we will use the simple Naïve Bayes algorithm.\n",
    "This is a _binary classification problem_ because we are separating instances of data into two categories, and since we have labeled data to train on, this is an example of _supervised learning_. Naïve Bayes can also be used for multi-class classification (more than two categories to sort the data into), but we restrict ourselves to the binary case for ease of exposition. We will first develop the algorithm generally, then show how it applies specifically to the Titanic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $P$ be a probability measure and let $A$ and $B$ be events in the probability space. _Bayes' rule_ is the statement\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n",
    "$$\n",
    "\n",
    "Suppose we have $n$ features (kinds of data to train on) $X_1,\\ldots,X_n$ and let $Y$ denote the label corresponding to these features. We use Bayes' rule to write\n",
    "\n",
    "$$\n",
    "P(Y=i\\mid X_1,\\ldots,X_n) = \\frac{P(X_1,\\ldots,X_n\\mid Y=i)P(Y=i)}{P(X_1,\\ldots,X_n)}.\n",
    "$$\n",
    "\n",
    "The left-hand side is the probability that the tuple of data $(X_1,\\ldots,X_n)$ belongs to the class $Y = i$.\n",
    "The right-hand side has three elements:\n",
    "- The probability $P(Y=i)$ is called the _prior_, which is our initial guess for the probability of an arbitrary set of data belonging to the class $Y = i$.\n",
    "- The denominator $P(X_1,\\ldots,X_n)$ is called the _evidence_, which is the normalization factor for the conditional probability.\n",
    "- The conditional probability $P(X_1,\\ldots,X_n \\mid Y=i)$ is the _likelihood_, the probability of observing this specific data given that it belongs to the class $Y = i$. This is usually the trickiest part to compute in a Bayesian inference problem, but we will make an assumption that makes it easy to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To deal with the evidence, we use with the _Law of Total Probability_:\n",
    "\n",
    "$$\n",
    "P(X_1,\\ldots,X_n) = \\sum_{i\\in\\{0,1\\}} P(X_1,\\ldots,X_n|Y=i)P(Y=i).\n",
    "$$\n",
    "\n",
    "In other words, the evidence can be written in terms of the likelihoods and priors. To compute the likelihood, we use the (naïve!) assumption that **the data features are conditionally independent**, that is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(X_1,\\ldots,X_n|Y=i)\n",
    "    &= \\prod_{j=1}^{n}P(X_{j}|Y = i)\n",
    "    \\\\\n",
    "    &= P(X_1|Y=i)P(X_2|Y=i)\\cdots P(X_n|Y=i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Real-world data rarely satisfies this assumption, but in many cases it is a sufficient approximation to get good results. In any case, this assumption greatly simplifies the mathematical and computational aspects of the algorithm, since no joint conditional probabilities need to be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To compute each $P(X_j|Y=i)$, we need to choose probability distributions for each $X_j$. We'll give two examples.\n",
    "1. If $X_1$ can take on two values, say 0 and 1, then we choose a Bernoulli distribution to model the probability distribution of $X_1$:\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(X_1 = 1 \\mid Y = i) &= q_i,\n",
    "    &\n",
    "    P(X_1 = 0 \\mid Y = i) &= 1 - q_i.\n",
    "\\end{align*}\n",
    "$$\n",
    "<!--  -->\n",
    "We estimate $q_i$ from the training data by counting the number of instances where $X_1 = 1$ among the group where $Y = i$. That is,\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q_i = \\frac{\\textrm{# data points with}\\ X_1 = 1, Y = i}{\\textrm{total # of data points with}\\ Y = i}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. For a continuous feature, we need to select and calibrate a continuous distribution. For example, we may assume $X_2$ has a Gaussian (Normal) distribution, defined by the mean $\\mu$ and standard deviation $\\sigma$. To estimate $\\mu$ and $\\sigma$ for each group, we again use the training data:\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_{i} &= \\textrm{mean}\\left(\\textrm{values of}\\ X_2\\ \\textrm{where}\\ Y = i\\right)\n",
    "    \\\\\n",
    "    \\sigma_{i} &= \\textrm{standard deviation}\\left(\\textrm{values of}\\ X_2\\ \\textrm{where}\\ Y = i\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "<!--  -->\n",
    "We then approximate $P(X_2 \\mid Y = i)$ by evaluating the probability density function of the normal distribution $\\mathcal{N}(\\mu,\\sigma)$ at the value of $X_2$. We aren't computing an actual probability here, but the strategy works because we only care about the relative plausibility of $X_2 \\mid Y=i$ compared to $X_2 \\mid Y\\neq i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can compute $P(Y = i \\mid X_1,\\ldots,X_n)$ for any sample of data $X_1,\\ldots,X_n$ via Bayes' rule. To classify the data, we simply pick the most probable label.\n",
    "\n",
    "$$\n",
    "    \\textrm{label of data}\\ (X_1,\\ldots,X_n)\n",
    "    = \\underset{i\\in\\{0,1\\}}{\\textrm{argmax}}\\ P(Y = i \\mid X_1,\\ldots,X_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros and Cons of Naïve Bayes\n",
    "\n",
    "##### Pros\n",
    "- The algorithm is simple and easy to implement, as we show below.\n",
    "- The approach is highly scalable, meaning it's cheap to do for really large data sets. This is the biggest advantage of the conditional indepencence assumption.\n",
    "- The results are highly interpretable: first, we get probabilities for each label---not just prediction labels---so the uncertainty of the prediction is quantified; second, we may examine each of the learned likelihood distributions to understand the algorithm's decision-making process.\n",
    "- Many machine learning algorithms require some kind of data preprocessing (shifting, scaling, etc.) so that the features can be compared fairly. Since Naïve Bayes treats the features independently, no preprocessing is required.\n",
    "- Because the algorithm makes decisions by learning distributions, it is robust to outliers in the training data.\n",
    "\n",
    "##### Cons\n",
    "- The assumption that the data features are conditionally independent is almost never true in the wild. The algorithm is highly susceptible to failure in situations where several of the data features are strongly (or even loosely) related. \n",
    "- We have to assign a probability distribution for each feature, and the effectiveness on the algorithm depends largely on how well the distributions we pick actually represent the data. Naïve Bayes works well in statisically simple situations (all variables are Gaussian), but it does not work as well in more statistically complicated situations. \n",
    "- To learn appropriate distributions for each feature we need lots of data. Most machine learning algorithms thrive on more data, but Naïve Bayes is particularly sensitive to sparse data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Application to the Titanic Data\n",
    "\n",
    "We want the probability of survival given the features in the data, so we want to compute\n",
    "\n",
    "$$\n",
    "P(\\textrm{Survived} \\mid \\textrm{Sex,Age,Pclass,Fare})\n",
    "= \\frac{P(\\textrm{Sex,Age,Pclass,Fare} \\mid \\textrm{Survived})P(\\textrm{Survived})}{P(\\textrm{Sex,Age,Pclass,Fare})}.\n",
    "$$\n",
    "\n",
    "To build a Naïve Bayes classifier, we need to choose distributions for each of the data features.\n",
    "- $X_1$ = Sex. In the data this is a binary variable (`male` or `female`), so we model it with a [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) distribution as in the example above.\n",
    "- $X_2$ = Age. This is a continuous variable, and as we saw in the brief data summary, we can approximate this fairly well with a [normal](https://en.wikipedia.org/wiki/Normal_distribution) (Gaussian) distribution.\n",
    "- $X_3$ = Pclass. There are three possible values in the data (1, 2, or 3), so we choose a discrete distribution as with Sex. In this case, since there are three choices, we use a [categorical](https://en.wikipedia.org/wiki/Categorical_distribution) distribution (Bernoulli but with more than two possible outcomes).\n",
    "- $X_4$ = Fare. Like age, fare is a continuous variable, but based on our brief data exploration, a normal distribution would be a poor approximation for this data. We use instead an [exponential](https://en.wikipedia.org/wiki/Exponential_distribution) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class TitanicNaiveBayes:\n",
    "    def fit(self, data):\n",
    "        groups = data.groupby(\"Survived\")\n",
    "        \n",
    "        sex_params = {}\n",
    "        age_params = {}\n",
    "        fare_params = {}\n",
    "        class_params = {}\n",
    "        \n",
    "        for i in range(0, 2):\n",
    "            group = groups.get_group(i)\n",
    "            \n",
    "            # Get the average number of males in the group\n",
    "            num_males = len(group[group[\"Sex\"] == \"male\"])\n",
    "            num_females = len(group[group[\"Sex\"] == \"female\"])\n",
    "            total_group = len(group)\n",
    "            male_ratio = num_males / total_group\n",
    "            female_ratio = num_females / total_group\n",
    "            sex_params[i] = (male_ratio, female_ratio)\n",
    "            \n",
    "            # Get the mean and standard deviation of the ages in the group\n",
    "            age_mean = group[\"Age\"].mean()\n",
    "            age_std = group[\"Age\"].std()\n",
    "            age_params[i] = (age_mean, age_std)\n",
    "            \n",
    "            # Get the mean and standard deviation of the fare in the group\n",
    "            loc, scale = stats.expon.fit(group[\"Fare\"].dropna())\n",
    "            fare_params[i] = (loc, scale)\n",
    "            \n",
    "            # Get the probability for each class in the group\n",
    "            num_first = len(group[group[\"Pclass\"] == 1])\n",
    "            num_second = len(group[group[\"Pclass\"] == 2])\n",
    "            num_third = len(group[group[\"Pclass\"] == 3])\n",
    "            first_ratio = num_first / total_group\n",
    "            second_ratio = num_second / total_group\n",
    "            third_ratio = num_third / total_group\n",
    "            class_params[i] = (first_ratio, second_ratio, third_ratio)\n",
    "        \n",
    "        self.sex_params_ = sex_params\n",
    "        self.age_params_ = age_params\n",
    "        self.fare_params_ = fare_params\n",
    "        self.class_params_ = class_params\n",
    "        return self\n",
    "    \n",
    "    def proba(self, data):\n",
    "        results = []\n",
    "        for i in range(0, len(data)):\n",
    "            result = []\n",
    "            for k in range(0, 2):\n",
    "                \n",
    "                p1 = 1.0\n",
    "                male, female = self.sex_params_[k]\n",
    "                if data.iloc[i, 0] == \"male\":\n",
    "                    p1 = male\n",
    "                else:\n",
    "                    p1 = female\n",
    "                \n",
    "                mean, std = self.age_params_[k]\n",
    "                p2 = stats.norm(mean, std).pdf(data.iloc[i, 1])\n",
    "                \n",
    "                p3 = 1.0\n",
    "                first, second, third = self.class_params_[k]\n",
    "                p_class = int(data.iloc[i, 2])\n",
    "                if p_class == 1:\n",
    "                    p3 = first\n",
    "                elif p_class == 2:\n",
    "                    p3 = second\n",
    "                else:\n",
    "                    p3 = third\n",
    "                \n",
    "                loc, scale = self.fare_params_[k]\n",
    "                p4 = stats.expon(loc, scale).pdf(data.iloc[i, 3])\n",
    "                \n",
    "                result.append(np.exp(np.sum(np.log([p1, p2, p3, p4, .5]))))\n",
    "            denom = sum(result)\n",
    "            results.append(np.array(result) / denom)\n",
    "        return np.array(results)\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict a survival category for each row of data via Bayes' rule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : (m,) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.proba(data), axis=-1)\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        \"\"\"Compute the accuracy of the model given some test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            The % of rows guessed correctly.\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(data) == labels)\n",
    "    \n",
    "    # Distribution plots ------------------------------------------------\n",
    "    def _survival_label(self, Y):\n",
    "        return \"Perished\" if Y == 0 else \"Survived\"\n",
    "\n",
    "    def plot_prior(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Prior\"])\n",
    "        data[\"Prior\"] = self.prior_.pmf([0,1])\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend([])\n",
    "        plt.title(\"Prior Survival Distribution\")\n",
    "\n",
    "    def plot_sex(self):\n",
    "        sexes = np.array([0, 1])\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Female\", \"Male\"])\n",
    "        for k in range(0, 2):\n",
    "            male, female = self.sex_params_[k]\n",
    "            data.iloc[k, 0] = female\n",
    "            data.iloc[k, 1] = male\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Sex Distributions\")\n",
    "\n",
    "    def plot_age(self):\n",
    "        years = np.linspace(0, 100, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(years, distributions[\"Age\"].pdf(years),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Age Distributions\")\n",
    "\n",
    "    def plot_fare(self):\n",
    "        fares = np.linspace(0, 550, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(fares, distributions[\"Fare\"].pdf(fares),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Fare Distributions\")\n",
    "\n",
    "    def plot_pclass(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"1st Class\", \"2nd Class\", \"3rd Class\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Pclass\"]\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Pclass Distributions\")\n",
    "\n",
    "    def plot_likelihoods(self):\n",
    "        \"\"\"Plot each of the learned distributions.\"\"\"\n",
    "        self.plot_prior()\n",
    "        plt.show()\n",
    "        self.plot_sex()\n",
    "        plt.show()\n",
    "        self.plot_age()\n",
    "        plt.show()\n",
    "        self.plot_fare()\n",
    "        plt.show()\n",
    "        self.plot_pclass()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TitanicNaiveBayes2:\n",
    "    \"\"\"Naïve Bayes classifier for the Titanic problem, mapping values of\n",
    "    Sex, Age, Fare, and Pclass to Survival (0 = perished, 1 = survived).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    prior_ : scipy.stats Bernoulli distribution\n",
    "        Survival probability prior, i.e., prior_.pmf(Y) returns the\n",
    "        probability of perishing if Y = 0 or surviving if Y = 1.\n",
    "\n",
    "    likelihood_ : dict[Y -> scipy.stats distributions]\n",
    "        Independent likelihood probability distributions whose parameters\n",
    "        are computed from the training data. In order:\n",
    "        * Sex: Bernoulli\n",
    "        * Age: Gaussian (normal)\n",
    "        * Fare: Exponential\n",
    "        * Pclass: Categorical\n",
    "    \"\"\"\n",
    "    def __init__(self, survival_prior=.5):\n",
    "        \"\"\"Set the probability of survival (Benoulli prior distribution).\"\"\"\n",
    "        self.prior_ = stats.bernoulli(survival_prior)\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Calculate statistics for each group based on survival.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Data to train on. Must have columns\n",
    "            [\"Survived\", \"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        groups = data.groupby(\"Survived\")\n",
    "        self.likelihood_ = {}\n",
    "\n",
    "        for Y, group in groups:\n",
    "            # Record the label.\n",
    "            distributions = {}\n",
    "\n",
    "            # Sex distribution: Bernoulli.\n",
    "            q = np.mean(group[\"Sex\"] == \"male\")\n",
    "            distributions[\"Sex\"] = stats.bernoulli(q)\n",
    "\n",
    "            # Age distribution: Gaussian (Normal).\n",
    "            µ, σ = group[\"Age\"].mean(), group[\"Age\"].std()\n",
    "            distributions[\"Age\"] = stats.norm(µ, σ)\n",
    "\n",
    "            # Fare distribution: Exponential.\n",
    "            loc, scale = stats.expon.fit(group[\"Fare\"].dropna())\n",
    "            distributions[\"Fare\"] = stats.expon(loc, scale)\n",
    "\n",
    "            # Pclass distribution: Categorical.\n",
    "            qs = np.array([np.mean(group[\"Pclass\"] == j) for j in [1,2,3]])\n",
    "            distributions[\"Pclass\"] = qs\n",
    "\n",
    "            self.likelihood_[Y] = distributions\n",
    "\n",
    "        return self\n",
    "\n",
    "    def proba(self, data):\n",
    "        \"\"\"Calculate the probabilites of each row of data belonging to each\n",
    "        survival category via Bayes' rule:\n",
    "\n",
    "                                        P(features | Survived=i) P(Survived)\n",
    "        P(Survived=i | features) = ---------------------------------------------\n",
    "                                   sum_{j}[P(features | Survived=j) P(Survived)]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : (m,2) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        numerators = []\n",
    "        for Y in self.likelihood_:\n",
    "            dist = self.likelihood_[Y]\n",
    "\n",
    "            # Evaluate the likelihood and prior distributions.\n",
    "            pXs = [dist[\"Sex\"].pmf(data[\"Sex\"] == \"male\"),        # P(Sex|Y)\n",
    "                   dist[\"Age\"].pdf(data[\"Age\"]),                  # P(Age|Y)\n",
    "                   dist[\"Fare\"].pdf(data[\"Fare\"]),                # P(Fare|Y)\n",
    "                   dist[\"Pclass\"][np.int32(data[\"Pclass\"]) - 1],  # P(Pclass|Y)\n",
    "                   self.prior_.pmf([Y]*len(data))                 # P(Y)\n",
    "                   ]\n",
    "\n",
    "            numerators.append(np.exp(np.sum(np.log(pXs), axis=0)))\n",
    "        numers = np.array(numerators)\n",
    "\n",
    "        return np.transpose(numers / np.sum(numers, axis=0))\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict a survival category for each row of data via Bayes' rule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : (m,) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.proba(data), axis=-1)\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        \"\"\"Compute the accuracy of the model given some test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            The % of rows guessed correctly.\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(data) == labels)\n",
    "\n",
    "    # Distribution plots ------------------------------------------------\n",
    "    def _survival_label(self, Y):\n",
    "        return \"Perished\" if Y == 0 else \"Survived\"\n",
    "\n",
    "    def plot_prior(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Prior\"])\n",
    "        data[\"Prior\"] = self.prior_.pmf([0,1])\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend([])\n",
    "        plt.title(\"Prior Survival Distribution\")\n",
    "\n",
    "    def plot_sex(self):\n",
    "        sexes = np.array([0, 1])\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Female\", \"Male\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Sex\"].pmf(sexes)\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Sex Distributions\")\n",
    "\n",
    "    def plot_age(self):\n",
    "        years = np.linspace(0, 100, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(years, distributions[\"Age\"].pdf(years),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Age Distributions\")\n",
    "\n",
    "    def plot_fare(self):\n",
    "        fares = np.linspace(0, 550, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(fares, distributions[\"Fare\"].pdf(fares),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Fare Distributions\")\n",
    "\n",
    "    def plot_pclass(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"1st Class\", \"2nd Class\", \"3rd Class\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Pclass\"]\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Pclass Distributions\")\n",
    "\n",
    "    def plot_likelihoods(self):\n",
    "        \"\"\"Plot each of the learned distributions.\"\"\"\n",
    "        self.plot_prior()\n",
    "        plt.show()\n",
    "        self.plot_sex()\n",
    "        plt.show()\n",
    "        self.plot_age()\n",
    "        plt.show()\n",
    "        self.plot_fare()\n",
    "        plt.show()\n",
    "        self.plot_pclass()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before we use Naïve Bayes on the Titanic data, we should check the assumption of conditional independence among the training variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Are these variables correlated?\n",
    "train_data.drop(\"Survived\", axis=1).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_This is why we \\*might\\* have a chance with Naïve Bayes, it looks like the features aren't too interdependent._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = TitanicNaiveBayes().fit(train_data)\n",
    "nb.plot_sex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.accuracy(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_data.sample(5)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.proba(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[samples.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[np.argmax(nb.proba(test_data), axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this presentation, we gave an overview of the Naïve Bayes algorithm and implemented it specifically for the Titanic survival problem. With only a limited number of data features in the prediction we obtained reasonable results. However, more sophisticated algorithms (or using more features from the data) would likely outperform our implementation since the features do not satisfy the conditional independence assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: More Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Pclass\").count()[\"Age\"].plot(kind=\"barh\")\n",
    "plt.title(\"Passenger Count by Ticket Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above shows the number of passengers in the data that are in each class based on the ticket. From the bar graph, the majority of passengers, recorded in the data, are in 3rd class followed by 1st and 2nd class by a relatively small gap between the two higher classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table gives detailed summary statistics for each of the featues in the training data. From the table, we can see that: \n",
    "- The mean age of the passsengers in the Titanic was 29-30 years old, but the median age is about 28 years old, which confirms the right-skewness of the distribution of ages. The youngest passenger was a baby while the oldest passenger is 80 years old. \n",
    "- A good majority of passengers recorded bought 3rd class tickets though the mean (average) was is about 2nd class. \n",
    "- The mean fare of the passengers in the Titanic was about \\\\$33.50 dollars, but the median fare was only about \\\\$14.50, dollars which confirms the extreme right-skewness of the distribution of fare. The cheapest ticket was for free while the most expensive ticket was about 512.33 dollars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows all the passengers in the data whose ages are unknown so far. According to the table, there are 263 entries or passengers whose ages were not recorded and one passenger with a missing ticket fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Survived Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_groups = titanic.groupby(\"Survived\")\n",
    "survival_groups.boxplot(grid=False, column=['Age'], layout=(1,2))\n",
    "survival_groups.boxplot(grid=False, column=['Fare'], layout=(1,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on survival/non-survival. While it may seem that survivors may be slightly older and bought slightly more expensive tickets on average compared to their non-survivor counterparts, there are a lot of outliers all the boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pclass Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = titanic.groupby(\"Pclass\")\n",
    "groups.boxplot(grid=False, column=['Age'], layout=(1,3))\n",
    "groups.boxplot(grid=False, column=['Fare'], layout=(1,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on Pclass (1, 2, 3). \n",
    "\n",
    "There are no outliers in the distribution of age in the 1st class, but there are many outliers in the other classes in terms of age, especially 3rd class passengers since there is more data collected for 3rd class compared to the 2 higher classes. The average age of 1st class passengers is older than the other two classes, and the average age decreases as we get lower in class. \n",
    "\n",
    "There are outliers in the distributions of fare for all classes, especially 1st class since we expect that more expensive tickets are brought through this class. There are a lot of outliers for the distribution of fare for 3rd class passengers not just only because we expect the cheapest tickets from this class, but also how there is more data collected from this class as well. The average fare of 1st class passengers is obviously higher/more expensive than the other two classes, and the average fare decreases as we get lower in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sex Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_groups = titanic.groupby(\"Sex\")\n",
    "gender_groups.boxplot(grid=False, column=['Age'], layout=(1,2))\n",
    "gender_groups.boxplot(grid=False, column=['Fare'], layout=(1,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on sex (male/female). There is only one outlier in the age distribution of females while there are multiple outliers in the age distribution of males. The average age of males and females is about the same, but with average age of males slightly higher. Then, there are a lot of outliers in the female and male distributions of fare. The average fare of males is slightly lower than females. Since passengers of cheaper tickets or older age are more likely to perish in the Titanic, it could be possible that sex played a significant role in who gets to survive and who doesn't in the Titanic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age and Fare as Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the passengers into 3 categories based on age.\n",
    "age = pd.cut(titanic['Age'], [0, 12, 18, 80])\n",
    "age\n",
    "\n",
    "titanic.pivot_table(values='Survived', index=['Sex', age],\n",
    "                   columns='Pclass', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of survival rate between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and Pclass (1, 2, 3). (Sex + Age vs PClass / PClass vs Sex + Age)\n",
    "\n",
    "Groups with everyone surviving: \n",
    "- 2nd class female children\n",
    "- 1st class female teenagers\n",
    "- 1st class male children\n",
    "- 2nd class male children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(values='Survived', index=['Sex', age],\n",
    "                   columns='Pclass', aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of number of survivors between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and Pclass (1, 2, 3). (Sex + Age vs PClass / PClass vs Sex + Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the ticket price into two intervals (cheap vs expensive)\n",
    "fare = pd.qcut(titanic['Fare'], 2)\n",
    "fare\n",
    "\n",
    "titanic.pivot_table(values='Survived',\n",
    "                   index=['Sex', age], columns=[fare, 'Pclass'],\n",
    "                   aggfunc='count', fill_value='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of number of survivors between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and fare divided into intervals (cheap (0-14.454) and expensive-ish (14.454, 512.329)) and then Pclass (1, 2, 3). (Sex + Age vs Fare + PClass / Fare + PClass vs Sex + Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age and Fare Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.plot(kind='scatter', x='Age', y='Fare', alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weak correlation between age and fare based on the graph above. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
