{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"figure\", dpi=300, figsize=(9,3))\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Titanic Survival Estimation via Naïve Bayes</h1>\n",
    "<h2 align=\"center\">Angela Cao</h2>\n",
    "<h5 align=\"center\">Fall 2020 Directed Readings Program, University of Texas at Austin Department of Mathematics</h4>\n",
    "<h5 align=\"center\">Advised by Shane McQuarrie</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [**Introduction: The Titanic Problem**](#Introduction:-The-Titanic-Problem)\n",
    "2. [**Brief Data Summary**](#Brief-Data-Summary)\n",
    "3. [**The Naïve Bayes Algorithm**](#The-Naïve-Bayes-Algorithm)\n",
    "4. [**Applying the Algorithm**](#Applying-the-Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: The Titanic Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On April 15, 1912, the [RMS Titanic](https://en.wikipedia.org/wiki/Titanic) sank, leaving about 1,500 people dead. The goal of this project is to construct a machine learning algorithm to predict whether or not a person with given characteristics would have been likely to survive the accident. This is a [popular problem](https://www.kaggle.com/c/titanic) for a first-time exposure to machine learning.\n",
    "\n",
    "We have a [data set](https://www.kaggle.com/c/titanic/data) with information about 1,308 of the Titanic passengers, with the following features for each passenger. We begin by loading the data and displaying a few random entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from file.\n",
    "titanic_original = pd.read_csv(\"titanic.csv\")\n",
    "titanic_original.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these features differentiate a survivor from a nonsurvivor? We restrict our attention to the following features:\n",
    "- `Sex`: the sex of the passenger, either `male` or `female`.\n",
    "- `Age`: the age of the passenger, usually recorded as a positive integer.\n",
    "- `Pclass`: the class of the passenger's ticket, 1 for first class (the best), 2 for second class, and 3 for third class (the worst).\n",
    "- `Fare`: the cost of the passenger's ticket.\n",
    "\n",
    "Some of the other columns in the data may be useful, but certainly these features are more important survival indicators than, for example, the passenger `Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns.\n",
    "titanic = titanic_original[[\"Survived\", \"Sex\", \"Age\", \"Pclass\", \"Fare\"]]\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any analysis, we split the data into training and testing sets, reserving 15% of the entries for testing. We are only allowed to use the testing set to evaluate the performance of our algorithm later on. Since some of the entries of the data may be missing (for example, some passengers don't have a recorded age), we replace missing values of the test data with the column averages of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "train_data, test_data = train_test_split(titanic, test_size=.15)\n",
    "print(f\"{len(train_data)} train points, {len(test_data)} test points ({len(titanic)} total)\")\n",
    "\n",
    "# Separate the Survival labels from the test data and replace\n",
    "# missing values with the averages from the training data.\n",
    "test_labels = test_data[\"Survived\"]\n",
    "test_data = test_data.drop(\"Survived\", axis=1).fillna(train_data.mean())\n",
    "\n",
    "test_data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before describing the algorithm, we give a brief statistical overview of the training data (remember, we aren't allowed to even look at the testing data). First, what was the rate of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 2))\n",
    "\n",
    "survival_count = train_data.groupby(\"Survived\").count().max(axis=1)\n",
    "survival_count.plot(kind=\"barh\", ax=ax)\n",
    "\n",
    "ax.set_ylabel(\"\"); ax.set_yticklabels([\"perished\", \"survived\"])\n",
    "ax.set_xlabel(\"number of passengers\")\n",
    "ax.set_title(f\"Passenger Survival Rate: {survival_count[1] / sum(survival_count):.2%}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above conveys the number of passengers in the data who survived or perished. From the bar graph above, more passengers, according to the data, perished, and the passenger survival rate is 38.13%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(9,2))\n",
    "\n",
    "train_data[\"Age\"].plot(kind=\"hist\", bins=80, ax=ax1)\n",
    "ax1.set_xlabel(\"passenger age\")\n",
    "\n",
    "train_data[\"Fare\"].plot(kind=\"hist\", bins=40, ax=ax2)\n",
    "ax2.set_ylabel(\"\"); ax2.set_xlabel(\"ticket fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram on the left shows the distribution of the ages of the passengers in the training data. The distribution is slightly right-skewed with a good amount of passengers in their 20's and 30's and also a good chunk of children as well.\n",
    "\n",
    "The histogram on the right shows the distribution of the ticket prices of the passengers in the training data. This distribution is extremely right-skewed, with the overwhelming majority of passengers having tickets that cost \\$100  or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we briefly look for statistical differences between survivors and nonsurvivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby([\"Survived\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table sorts the data by the survival label (0 for non-survivors and 1 for survivors), then computes mean of the columns for each group. A few things are immediate apparent:\n",
    "- Survivors had, on average, significantly more expensive tickets than non-survivors.\n",
    "- Survivors, on average, were in a (slightly) higher class than non-survivors; non-survivors were highly likely to be in a lower or common class (like third-class). \n",
    "- Survivors, on average, were slightly younger than non-survirors. \n",
    "\n",
    "In short, the survivors and non-survivors exhibit different statistics; we aim to use that information to construct a \"survival classifier.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naïve Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to construct a function that maps a given tuple of `Sex`, `Age`, `Pclass`, and `Fare` values to a `Survival` value (0 for perished or 1 for survived).\n",
    "There are many ways to construct such a function, but we will use the simple Naïve Bayes algorithm.\n",
    "This is a _binary classification problem_ because we are separating instances of data into two categories, and since we have labeled data to train on, this is an example of _supervised learning_. Naïve Bayes can also be used for multi-class classification (more than two categories to sort the data into), but we restrict ourselves to the binary case for ease of exposition. We will first develop the algorithm generally, then show how it applies specifically to the Titanic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $P$ be a probability measure and let $A$ and $B$ be events in the probability space. _Bayes' rule_ is the statement\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}.\n",
    "$$\n",
    "\n",
    "Suppose we have $n$ features (kinds of data to train on) $X_1,\\ldots,X_n$ and let $Y$ denote the label corresponding to these features. We use Bayes' rule to write\n",
    "\n",
    "$$\n",
    "P(Y=i\\mid X_1,\\ldots,X_n) = \\frac{P(X_1,\\ldots,X_n\\mid Y=i)P(Y=i)}{P(X_1,\\ldots,X_n)}.\n",
    "$$\n",
    "\n",
    "The left-hand side is the probability that the tuple of data $(X_1,\\ldots,X_n)$ belongs to the class $Y = i$.\n",
    "The right-hand side has three elements:\n",
    "- The probability $P(Y=i)$ is called the _prior_, which is our initial guess for the probability of an arbitrary set of data belonging to the class $Y = i$.\n",
    "- The denominator $P(X_1,\\ldots,X_n)$ is called the _evidence_, which is the normalization factor for the conditional probability.\n",
    "- The conditional probability $P(X_1,\\ldots,X_n \\mid Y=i)$ is the _likelihood_, the probability of observing this specific data given that it belongs to the class $Y = i$. This is usually the trickiest part to compute in a Bayesian inference problem, but we will make an assumption that makes it easy to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the evidence, we use with the _Law of Total Probability_:\n",
    "\n",
    "$$\n",
    "P(X_1,\\ldots,X_n) = \\sum_{i\\in\\{0,1\\}} P(X_1,\\ldots,X_n|Y=i)P(Y=i).\n",
    "$$\n",
    "\n",
    "In other words, the evidence can be written in terms of the likelihoods and priors. To compute the likelihood, we use the (naïve!) assumption that **the data features are conditionally independent**, that is,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(X_1,\\ldots,X_n|Y=i)\n",
    "    &= \\prod_{j=1}^{n}P(X_{j}|Y = i)\n",
    "    \\\\\n",
    "    &= P(X_1|Y=i)P(X_2|Y=i)\\cdots P(X_n|Y=i)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**_Write a little bit here about why this may or may not be a good idea._**\n",
    "\n",
    "Naïve Bayes might not always be a good algorithm to implement, because we're assuming that all the elements involved in the algorithm are independent to each other which might not always be the case (ex. there might be a correlation between fare (ticket) price and age or p-class). Also, the algorithm might not work for more statisically-complicated situations (one of them is if all the elements are not independent from each other) as well. We're oversimplying the situation where the algorithm is applied to and the math behind the situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute each $P(X_j|Y=i)$, we need to choose probability distributions for each $X_j$. We'll give two examples.\n",
    "1. If $X_1$ can take on two values 0 and 1 (e.g., in the titanic data, `Sex` is either `male` or `female`), then we choose a Bernoulli distribution to model the probability distribution of $X_1$, that is,\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(X_1 = 1 \\mid Y = i) &= q_i\n",
    "    &\n",
    "    P(X_1 = 0 \\mid Y = i) &= 1 - q_i\n",
    "\\end{align*}\n",
    "$$\n",
    "<!--  -->\n",
    "We estimate $q_i$ from the training data by counting the number of instances where $X_1 = 1$ among the group where $Y = i$. That is,\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    q_i = \\frac{\\textrm{# data points with}\\ X_1 = 1, Y = i}{\\textrm{total # of data points with}\\ Y = i}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. For a continuous feature, we need to select and calibrate a continuous distribution. For example, we may assume $X_2$ has a Gaussian (Normal) distribution, defined by the mean $\\mu$ and standard deviation $\\sigma$. To estimate $\\mu$ and $\\sigma$ for each group, we again use the training data:\n",
    "<!--  -->\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mu_{i} &= \\textrm{mean}\\left(\\textrm{values of}\\ X_2\\ \\textrm{where}\\ Y = i\\right)\n",
    "    \\\\\n",
    "    \\sigma_{i} &= \\textrm{standard deviation}\\left(\\textrm{values of}\\ X_2\\ \\textrm{where}\\ Y = i\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "<!--  -->\n",
    "We then compute $P(X_2 \\mid Y = i)$ by evaluating the probability density function of the normal distribution $\\mathcal{N}(\\mu,\\sigma)$ at the value of $X_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute $P(Y = i \\mid X_1,\\ldots,X_n)$ for any sample of data $X_1,\\ldots,X_n$. To classify the data, we simply pick the most probable label.\n",
    "\n",
    "$$\n",
    "    \\textrm{label of data}\\ (X_1,\\ldots,X_n)\n",
    "    = \\underset{i\\in\\{0,1\\}}{\\textrm{argmax}}\\ P(Y = i \\mid X_1,\\ldots,X_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and Cons\n",
    "\n",
    "**_Pros_**\n",
    "- It's _scalable_, meaning it's cheap to do for really large data sets.\n",
    "- It's highly interpretable: it gives you a probability for each class (so the uncertainty of the prediction is quantified) and you can look at each of the distributions it learns to understand its decision-making process.\n",
    "- It's easy to implement, as we see below.\n",
    "- It doesn't require any preprocessing like scaling.\n",
    "- The algorithm works efficiently, and it could give a relatively good approximation. \n",
    "- Robust to outliers since we're looking things at the distribution sense. \n",
    "\n",
    "**_Cons_**\n",
    "- For the algorithm to work effectively, all the variables must be independent to each other or we assume that they are independent to each other; will not work for a group of variables where several of the variables are interdependent on each other or the variables are not independent at all. \n",
    "- The algorithm will work on statisically simple situations (like if all variables are independent to each other for instance); will not work in situations where the relationship(s) is/are subtle or more statistically complicated situations. \n",
    "- Since the algorithm oversimplifies the statistical situation, accuracy is not always the best, especially in more interdependent or complicated situations.\n",
    "- You need a lot of data for the algorithm to work or else the distribution will not be good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to the Titanic Data\n",
    "\n",
    "We want the probability of survival given the features in the data, so we want to compute\n",
    "\n",
    "$$\n",
    "P(\\textrm{Survived} \\mid \\textrm{Sex,Age,Pclass,Fare})\n",
    "= \\frac{P(\\textrm{Sex,Age,Pclass,Fare} \\mid \\textrm{Survived})P(\\textrm{Survived})}{P(\\textrm{Sex,Age,Pclass,Fare})}.\n",
    "$$\n",
    "\n",
    "To build a Naïve Bayes classifier, we need to choose distributions for each of the data features.\n",
    "- $X_1$ = Sex. This is a binary variable, so we model it with a Bernoulli distribution.\n",
    "- $X_2$\n",
    "- $X_3$ Categorical (Bernoulli but with more than two possible outcomes)\n",
    "- $X_4$ Exponential (or Gaussian)\n",
    "\n",
    "**_Explain why we choose each of these (refer to the graphs in the data visualization section)_**\n",
    "\n",
    "Sex and Pclass are discrete variables. Sex is binary as the passenger is either male or female, and since it's a binary variable with two options of male or female, it can be used with a Bernoulli distribution. While not binary, Pclass is a discrete variable nonetheless: the passenger is either first class (1) or second class (2) or third/common class (3), and thus, can be considered as categorical. Age and fare, which are factors in determing survivor/non-survivor, are continuous variables, but as we see from the brief data summary, they have different distributions: age can be approximated with a Normal (Gaussian) distribution, but fare looks exponential. These factors can also determine which of the passengers of either sex or class survive since a mix of both sexes or all classes survived and perished (ex. some 2nd class passengers survived and some didn't; some females survived and some didn't). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNaiveBayes:\n",
    "    def fit(self, data):\n",
    "        groups = data.groupby(\"Survived\")\n",
    "        \n",
    "        sex_params = {}\n",
    "        age_params = {}\n",
    "        fare_params = {}\n",
    "        class_params = {}\n",
    "        \n",
    "        for i in range(0, 2):\n",
    "            group = groups.get_group(i)\n",
    "            \n",
    "            # Get the average number of males in the group\n",
    "            num_males = len(group[group[\"Sex\"] == \"male\"])\n",
    "            num_females = len(group[group[\"Sex\"] == \"female\"])\n",
    "            total_group = len(group)\n",
    "            male_ratio = num_males / total_group\n",
    "            female_ratio = num_females / total_group\n",
    "            sex_params[i] = (male_ratio, female_ratio)\n",
    "            \n",
    "            # Get the mean and standard deviation of the ages in the group\n",
    "            age_mean = group[\"Age\"].mean()\n",
    "            age_std = group[\"Age\"].std()\n",
    "            age_params[i] = (age_mean, age_std)\n",
    "            \n",
    "            # Get the mean and standard deviation of the fare in the group\n",
    "            loc, scale = stats.expon.fit(group[\"Fare\"].dropna())\n",
    "            fare_params[i] = (loc, scale)\n",
    "            \n",
    "            # Get the probability for each class in the group\n",
    "            num_first = len(group[group[\"Pclass\"] == 1])\n",
    "            num_second = len(group[group[\"Pclass\"] == 2])\n",
    "            num_third = len(group[group[\"Pclass\"] == 3])\n",
    "            first_ratio = num_first / total_group\n",
    "            second_ratio = num_second / total_group\n",
    "            third_ratio = num_third / total_group\n",
    "            class_params[i] = (first_ratio, second_ratio, third_ratio)\n",
    "        \n",
    "        self.sex_params_ = sex_params\n",
    "        self.age_params_ = age_params\n",
    "        self.fare_params_ = fare_params\n",
    "        self.class_params_ = class_params\n",
    "        return self\n",
    "    \n",
    "    def proba(self, data):\n",
    "        results = []\n",
    "        for i in range(0, len(data)):\n",
    "            result = []\n",
    "            for k in range(0, 2):\n",
    "                \n",
    "                p1 = 1.0\n",
    "                male, female = self.sex_params_[k]\n",
    "                if data.iloc[i, 0] == \"male\":\n",
    "                    p1 = male\n",
    "                else:\n",
    "                    p1 = female\n",
    "                \n",
    "                mean, std = self.age_params_[k]\n",
    "                p2 = stats.norm(mean, std).pdf(data.iloc[i, 1])\n",
    "                \n",
    "                p3 = 1.0\n",
    "                first, second, third = self.class_params_[k]\n",
    "                p_class = int(data.iloc[i, 2])\n",
    "                if p_class == 1:\n",
    "                    p3 = first\n",
    "                elif p_class == 2:\n",
    "                    p3 = second\n",
    "                else:\n",
    "                    p3 = third\n",
    "                \n",
    "                loc, scale = self.fare_params_[k]\n",
    "                p4 = stats.expon(loc, scale).pdf(data.iloc[i, 3])\n",
    "                \n",
    "                result.append(np.exp(np.sum(np.log([p1, p2, p3, p4, .5]))))\n",
    "            denom = sum(result)\n",
    "            results.append(np.array(result) / denom)\n",
    "        return np.array(results)\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict a survival category for each row of data via Bayes' rule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : (m,) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.proba(data), axis=-1)\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        \"\"\"Compute the accuracy of the model given some test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            The % of rows guessed correctly.\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(data) == labels)\n",
    "    \n",
    "    # Distribution plots ------------------------------------------------\n",
    "    def _survival_label(self, Y):\n",
    "        return \"Perished\" if Y == 0 else \"Survived\"\n",
    "\n",
    "    def plot_prior(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Prior\"])\n",
    "        data[\"Prior\"] = self.prior_.pmf([0,1])\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend([])\n",
    "        plt.title(\"Prior Survival Distribution\")\n",
    "\n",
    "    def plot_sex(self):\n",
    "        sexes = np.array([0, 1])\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Female\", \"Male\"])\n",
    "        for k in range(0, 2):\n",
    "            male, female = self.sex_params_[k]\n",
    "            data.iloc[k, 0] = female\n",
    "            data.iloc[k, 1] = male\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Sex Distributions\")\n",
    "\n",
    "    def plot_age(self):\n",
    "        years = np.linspace(0, 100, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(years, distributions[\"Age\"].pdf(years),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Age Distributions\")\n",
    "\n",
    "    def plot_fare(self):\n",
    "        fares = np.linspace(0, 550, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(fares, distributions[\"Fare\"].pdf(fares),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Fare Distributions\")\n",
    "\n",
    "    def plot_pclass(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"1st Class\", \"2nd Class\", \"3rd Class\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Pclass\"]\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Pclass Distributions\")\n",
    "\n",
    "    def plot_likelihoods(self):\n",
    "        \"\"\"Plot each of the learned distributions.\"\"\"\n",
    "        self.plot_prior()\n",
    "        plt.show()\n",
    "        self.plot_sex()\n",
    "        plt.show()\n",
    "        self.plot_age()\n",
    "        plt.show()\n",
    "        self.plot_fare()\n",
    "        plt.show()\n",
    "        self.plot_pclass()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNaiveBayes2:\n",
    "    \"\"\"Naïve Bayes classifier for the Titanic problem, mapping values of\n",
    "    Sex, Age, Fare, and Pclass to Survival (0 = perished, 1 = survived).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    prior_ : scipy.stats Bernoulli distribution\n",
    "        Survival probability prior, i.e., prior_.pmf(Y) returns the\n",
    "        probability of perishing if Y = 0 or surviving if Y = 1.\n",
    "\n",
    "    likelihood_ : dict[Y -> scipy.stats distributions]\n",
    "        Independent likelihood probability distributions whose parameters\n",
    "        are computed from the training data. In order:\n",
    "        * Sex: Bernoulli\n",
    "        * Age: Gaussian (normal)\n",
    "        * Fare: Exponential\n",
    "        * Pclass: Categorical\n",
    "    \"\"\"\n",
    "    def __init__(self, survival_prior=.5):\n",
    "        \"\"\"Set the probability of survival (Benoulli prior distribution).\"\"\"\n",
    "        self.prior_ = stats.bernoulli(survival_prior)\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Calculate statistics for each group based on survival.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            Data to train on. Must have columns\n",
    "            [\"Survived\", \"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        groups = data.groupby(\"Survived\")\n",
    "        self.likelihood_ = {}\n",
    "\n",
    "        for Y, group in groups:\n",
    "            # Record the label.\n",
    "            distributions = {}\n",
    "\n",
    "            # Sex distribution: Bernoulli.\n",
    "            q = np.mean(group[\"Sex\"] == \"male\")\n",
    "            distributions[\"Sex\"] = stats.bernoulli(q)\n",
    "\n",
    "            # Age distribution: Gaussian (Normal).\n",
    "            µ, σ = group[\"Age\"].mean(), group[\"Age\"].std()\n",
    "            distributions[\"Age\"] = stats.norm(µ, σ)\n",
    "\n",
    "            # Fare distribution: Exponential.\n",
    "            loc, scale = stats.expon.fit(group[\"Fare\"].dropna())\n",
    "            distributions[\"Fare\"] = stats.expon(loc, scale)\n",
    "\n",
    "            # Pclass distribution: Categorical.\n",
    "            qs = np.array([np.mean(group[\"Pclass\"] == j) for j in [1,2,3]])\n",
    "            distributions[\"Pclass\"] = qs\n",
    "\n",
    "            self.likelihood_[Y] = distributions\n",
    "\n",
    "        return self\n",
    "\n",
    "    def proba(self, data):\n",
    "        \"\"\"Calculate the probabilites of each row of data belonging to each\n",
    "        survival category via Bayes' rule:\n",
    "\n",
    "                                        P(features | Survived=i) P(Survived)\n",
    "        P(Survived=i | features) = ---------------------------------------------\n",
    "                                   sum_{j}[P(features | Survived=j) P(Survived)]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : (m,2) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        numerators = []\n",
    "        for Y in self.likelihood_:\n",
    "            dist = self.likelihood_[Y]\n",
    "\n",
    "            # Evaluate the likelihood and prior distributions.\n",
    "            pXs = [dist[\"Sex\"].pmf(data[\"Sex\"] == \"male\"),        # P(Sex|Y)\n",
    "                   dist[\"Age\"].pdf(data[\"Age\"]),                  # P(Age|Y)\n",
    "                   dist[\"Fare\"].pdf(data[\"Fare\"]),                # P(Fare|Y)\n",
    "                   dist[\"Pclass\"][np.int32(data[\"Pclass\"]) - 1],  # P(Pclass|Y)\n",
    "                   self.prior_.pmf([Y]*len(data))                 # P(Y)\n",
    "                   ]\n",
    "\n",
    "            numerators.append(np.exp(np.sum(np.log(pXs), axis=0)))\n",
    "        numers = np.array(numerators)\n",
    "\n",
    "        return np.transpose(numers / np.sum(numers, axis=0))\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict a survival category for each row of data via Bayes' rule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : (m,) ndarray\n",
    "            Perish / survival probabilities.\n",
    "        \"\"\"\n",
    "        return np.argmax(self.proba(data), axis=-1)\n",
    "\n",
    "    def accuracy(self, data, labels):\n",
    "        \"\"\"Compute the accuracy of the model given some test data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : (m,4) pd.DataFrame\n",
    "            Data to make a prediction for. Must have columns\n",
    "            [\"Sex\", \"Age\", \"Fare\", \"Pclass\"].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        accuracy : float\n",
    "            The % of rows guessed correctly.\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(data) == labels)\n",
    "\n",
    "    # Distribution plots ------------------------------------------------\n",
    "    def _survival_label(self, Y):\n",
    "        return \"Perished\" if Y == 0 else \"Survived\"\n",
    "\n",
    "    def plot_prior(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Prior\"])\n",
    "        data[\"Prior\"] = self.prior_.pmf([0,1])\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend([])\n",
    "        plt.title(\"Prior Survival Distribution\")\n",
    "\n",
    "    def plot_sex(self):\n",
    "        sexes = np.array([0, 1])\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"Female\", \"Male\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Sex\"].pmf(sexes)\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Sex Distributions\")\n",
    "\n",
    "    def plot_age(self):\n",
    "        years = np.linspace(0, 100, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(years, distributions[\"Age\"].pdf(years),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Age Distributions\")\n",
    "\n",
    "    def plot_fare(self):\n",
    "        fares = np.linspace(0, 550, 1000)\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            plt.plot(fares, distributions[\"Fare\"].pdf(fares),\n",
    "                     label=self._survival_label(Y))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Fare Distributions\")\n",
    "\n",
    "    def plot_pclass(self):\n",
    "        data = pd.DataFrame(index=[\"Perished\", \"Survived\"],\n",
    "                            columns=[\"1st Class\", \"2nd Class\", \"3rd Class\"])\n",
    "        for Y, distributions in self.likelihood_.items():\n",
    "            data.loc[self._survival_label(Y)] = distributions[\"Pclass\"]\n",
    "        data.plot(kind=\"barh\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Estimated Pclass Distributions\")\n",
    "\n",
    "    def plot_likelihoods(self):\n",
    "        \"\"\"Plot each of the learned distributions.\"\"\"\n",
    "        self.plot_prior()\n",
    "        plt.show()\n",
    "        self.plot_sex()\n",
    "        plt.show()\n",
    "        self.plot_age()\n",
    "        plt.show()\n",
    "        self.plot_fare()\n",
    "        plt.show()\n",
    "        self.plot_pclass()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Above are three different implementations of the Naive Bayes Algorithm in terms of the Titanic data.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use Naïve Bayes on the Titanic data, we should check the assumption of conditional independence among the training variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these variables correlated?\n",
    "train_data.drop(\"Survived\", axis=1).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_This is why we \\*might\\* have a chance with Naïve Bayes, it looks like the features aren't too interdependent._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = TitanicNaiveBayes().fit(train_data)\n",
    "nb.plot_sex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.accuracy(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_data.sample(5)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.proba(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[samples.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[np.argmax(nb.proba(test_data), axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this presentation, we gave an overview of the Naive Bayes algorithm, and implemented the algorithm for the Titanic problem. In the data, we decided to use a limited amount of columns or factors to determine the While Naive Bayes is not a sophisticated algorithm to use for the Titanic problem. However, we can do better with the same amount of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: More Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(\"Pclass\").count()[\"Age\"].plot(kind=\"barh\")\n",
    "plt.title(\"Passenger Count by Ticket Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above shows the number of passengers in the data that are in each class based on the ticket. From the bar graph, the majority of passengers, recorded in the data, are in 3rd class followed by 1st and 2nd class by a relatively small gap between the two higher classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only shows the amount of passengers recorded in the data through the 'Survived' column (1309 passengers recorded, and most of them probably perished according to the mean and quartiles), but also the number of passengers in the data who got their ages recorded and the mean and ranges of ages recorded, and the mean and ranges of classes and fares recorded as well through min, quartiles, and max. \n",
    "\n",
    "From the table, we can see that: \n",
    "- The majority of passengers recorded perished in the Titanic incident. \n",
    "- There were 1046 passengers whose ages were recorded in the data. Some passengers are missing ages. \n",
    "- The mean age of the passsengers in the Titanic were 29-30 years old. \n",
    "- The median age of the passengers in the Titanic is about 28 years old which confirms the right-skewness of the distribution of ages. \n",
    "- The youngest passenger was a baby while the oldest passenger is 80 years old. \n",
    "- A good majority of passengers recorded bought 3rd class tickets though the mean (average) was is about 2nd class. \n",
    "- The mean fare of the passengers in the Titanic was about 33.30 dollars. \n",
    "- The median fare of the passengers in the Titanic was about 14.45 dollars which confirms the extreme right-skewness of the distribution of fare. \n",
    "- The cheapest ticket was for free while the most expensive ticket was about 512.33 dollars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows all the passengers in the data whose ages are unknown so far. According to the table, there are 263 entries or passengers whose ages were not recorded and one passenger with a missing ticket fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Survived Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_groups = titanic.groupby(\"Survived\")\n",
    "survival_groups.boxplot(grid=False, column=['Age'], layout=(1,2), vert=False)\n",
    "survival_groups.boxplot(grid=False, column=['Fare'], layout=(1,2), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on survival/non-survival. While it may seem that survivors may be slightly older and bought slightly more expensive tickets on average compared to their non-survivor counterparts, there are a lot of outliers all the boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pclass Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = titanic.groupby(\"Pclass\")\n",
    "groups.boxplot(grid=False, column=['Age'], layout=(1,3))\n",
    "groups.boxplot(grid=False, column=['Fare'], layout=(1,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on Pclass (1, 2, 3). \n",
    "\n",
    "There are no outliers in the distribution of age in the 1st class, but there are many outliers in the other classes in terms of age, especially 3rd class passengers since there is more data collected for 3rd class compared to the 2 higher classes. The average age of 1st class passengers is older than the other two classes, and the average age decreases as we get lower in class. \n",
    "\n",
    "There are outliers in the distributions of fare for all classes, especially 1st class since we expect that more expensive tickets are brought through this class. There are a lot of outliers for the distribution of fare for 3rd class passengers not just only because we expect the cheapest tickets from this class, but also how there is more data collected from this class as well. The average fare of 1st class passengers is obviously higher/more expensive than the other two classes, and the average fare decreases as we get lower in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sex Classification Data Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_groups = titanic.groupby(\"Sex\")\n",
    "gender_groups.boxplot(grid=False, column=['Age'], layout=(1,2), vert=False)\n",
    "gender_groups.boxplot(grid=False, column=['Fare'], layout=(1,2), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box plots above show the distributions of age and fare based on sex (male/female). There is only one outlier in the age distribution of females while there are multiple outliers in the age distribution of males. The average age of males and females is about the same, but with average age of males slightly higher. Then, there are a lot of outliers in the female and male distributions of fare. The average fare of males is slightly lower than females. Since passengers of cheaper tickets or older age are more likely to perish in the Titanic, it could be possible that sex played a significant role in who gets to survive and who doesn't in the Titanic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age and Fare as Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the passengers into 3 categories based on age.\n",
    "age = pd.cut(titanic['Age'], [0, 12, 18, 80])\n",
    "age\n",
    "\n",
    "titanic.pivot_table(values='Survived', index=['Sex', age],\n",
    "                   columns='Pclass', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of survival rate between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and Pclass (1, 2, 3). (Sex + Age vs PClass / PClass vs Sex + Age)\n",
    "\n",
    "Groups with everyone surviving: \n",
    "- 2nd class female children\n",
    "- 1st class female teenagers\n",
    "- 1st class male children\n",
    "- 2nd class male children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(values='Survived', index=['Sex', age],\n",
    "                   columns='Pclass', aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of number of survivors between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and Pclass (1, 2, 3). (Sex + Age vs PClass / PClass vs Sex + Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the ticket price into two intervals (cheap vs expensive)\n",
    "fare = pd.qcut(titanic['Fare'], 2)\n",
    "fare\n",
    "\n",
    "titanic.pivot_table(values='Survived',\n",
    "                   index=['Sex', age], columns=[fare, 'Pclass'],\n",
    "                   aggfunc='count', fill_value='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a two-way table of number of survivors between sex and then age divided into intervals (children (0-12), teen (12-18), and adult (18-80)), and fare divided into intervals (cheap (0-14.454) and expensive-ish (14.454, 512.329)) and then Pclass (1, 2, 3). (Sex + Age vs Fare + PClass / Fare + PClass vs Sex + Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age and Fare Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.plot(kind='scatter', x='Age', y='Fare', alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weak correlation between age and fare based on the graph above, so a correlation between the two variables is not clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
